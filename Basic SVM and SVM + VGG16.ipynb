{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ae0292",
   "metadata": {},
   "source": [
    "# Basic SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ee39d",
   "metadata": {},
   "source": [
    "Import neccessary libray, load dataset, set categories name and preprocess image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c19dba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917\n"
     ]
    }
   ],
   "source": [
    "import os                         \n",
    "import numpy as np                \n",
    "import cv2                       \n",
    "import matplotlib.pyplot as plt   \n",
    "import pickle    \n",
    "data_dir = 'SVM folder/'\n",
    "categories = ['Avocado', 'Banana']\n",
    "data = []\n",
    "#Resharping image\n",
    "for category in categories:\n",
    "    #Create a new path which concatenate directory with category \n",
    "    path = os.path.join(data_dir, category)\n",
    "    #Determine the current index category in the list represent as numerical data\n",
    "    label = categories.index(category)\n",
    "    \n",
    "    for img in os.listdir(path):\n",
    "        imgpath = os.path.join(path, img)\n",
    "        #Read image\n",
    "        pet_img = cv2.imread(imgpath, 0)\n",
    "        try:\n",
    "            #Resize image to (50, 50)\n",
    "            pet_img = cv2.resize(pet_img, (50, 50))\n",
    "            #Converting from 2-dimensional to 1-dimensional array\n",
    "            #(Because SVM works with input data which is in vector form)\n",
    "            image = np.array(pet_img).flatten()\n",
    "            #Add image and label in Data\n",
    "            data.append([image, label])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "#Display size image in data   \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83d2c3",
   "metadata": {},
   "source": [
    "The same step but we can use tensorflow for preprocess image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0416c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "import numpy as np \n",
    "import pickle  \n",
    "data_dir = 'SVM folder/'\n",
    "categories = ['Avocado', 'Banana']\n",
    "img_height = 50\n",
    "img_width = 50\n",
    "\n",
    "# Load and preprocess the images\n",
    "data = []\n",
    "for category in categories:\n",
    "    path = os.path.join(data_dir, category)\n",
    "    label = categories.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        try:\n",
    "            # Load the image and resize it to (img_height, img_width)\n",
    "            img = load_img(img_path, color_mode='grayscale', target_size=(img_height, img_width))\n",
    "            # Convert the image to a numpy array and scale the pixel values to [0, 1]\n",
    "            image = np.array(img).flatten()\n",
    "            # Add the image and label to the data list\n",
    "            data.append((image, label))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# Display the number of images in the data list\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc1211",
   "metadata": {},
   "source": [
    "write process to file data.pickle and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03472bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to file after Resharping\n",
    "pick_in = open('data.pickle', 'wb')  #Write data\n",
    "pickle.dump(data, pick_in)            #Save data to pickle file\n",
    "pick_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6cd754",
   "metadata": {},
   "source": [
    "Read file that we store extract feature from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2af5a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Read data store in data \n",
    "pick_in = open('data.pickle', 'rb')\n",
    "data = pickle.load(pick_in)\n",
    "pick_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39849564",
   "metadata": {},
   "source": [
    "Initialize features and lables for store data from data.pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdc413eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Initialize features and labels\n",
    "random.shuffle(data) \n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edbd4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature and label from data store in each arrays\n",
    "for feature, label in data:\n",
    "    features.append(feature)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa90705",
   "metadata": {},
   "source": [
    "divid data for train and testing by using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaa2b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#50% of Data is token for testing\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size = 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa423ce",
   "metadata": {},
   "source": [
    "+ Build Model and Train data using svm.SVC() and .fit(xtrain, ytrain)\n",
    "+ Evaluate the classifier on the testing data \n",
    "+ Save model.sav for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a207db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 100.00%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "#Build Model and Model Training\n",
    "model = svm.SVC()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "accuracy = model.score(xtest, ytest)\n",
    "print(f'The accuracy of the classifier is {accuracy:.2%}.')\n",
    "\n",
    "#Save as model.sav\n",
    "pick = open('model.sav', 'wb')\n",
    "pickle.dump(model,  pick)\n",
    "pick.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43625565",
   "metadata": {},
   "source": [
    "This output high because we use small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e5b39",
   "metadata": {},
   "source": [
    "+ Load model.sav that sav svm model in previous step\n",
    "+ create prediction use this model and if we want to know accuracy object for test we can write model.score(xtest, ytest)\n",
    "+ set categories\n",
    "+ print accuracy and prediction\n",
    "+ show image of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "983b4145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Prediction:  Banana\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read Model for Prediction\n",
    "pick =  open('model.sav', 'rb')\n",
    "model = pickle.load(pick)\n",
    "pick.close()\n",
    "\n",
    "#Create prediction and accuracy object for test\n",
    "prediction = model.predict(xtest)\n",
    "accuracy = model.score(xtest, ytest)\n",
    "\n",
    "#Divide 2 Categories Dataset\n",
    "categories = ['Avocado', 'Banana']\n",
    "\n",
    "#Display the Accuracy and the prediction of animal\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Prediction: ', categories[prediction[0]])\n",
    "\n",
    "#Display pet image\n",
    "pet = xtest[0].reshape(50,50)\n",
    "plt.imshow(pet, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad5ff0",
   "metadata": {},
   "source": [
    "# For VGG16 + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a17232",
   "metadata": {},
   "source": [
    "Import neccessary library and load VGG16 model for extract feature for svm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bcf9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import glob\n",
    "# Load the VGG16 model\n",
    "model = VGG16()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25eccc",
   "metadata": {},
   "source": [
    "Load dataset and divide to images and labels of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d99725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = 'train/'\n",
    "img_paths = glob.glob(os.path.join(data_dir, '*', '*.jpg'))\n",
    "labels = [os.path.basename(os.path.dirname(img_path)) for img_path in img_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418ee7a",
   "metadata": {},
   "source": [
    "+ Resize image and then convert to numpy array\n",
    "  Reshape image data and put data to VGG16\n",
    "  Extract feature using VGG16\n",
    "  Add feature and its label to features[] and label_name[]\n",
    "+ Convert list of feature and lable_name to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4316dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "# Extract features from the dataset using VGG16 model\n",
    "features = []\n",
    "label_name = []\n",
    "for img_path, label in zip(img_paths, labels):\n",
    "    # Load the image\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Reshape the image data for the model\n",
    "    img_array = img_array.reshape((1, img_array.shape[0], img_array.shape[1], img_array.shape[2]))\n",
    "\n",
    "    # Preprocess the image for the VGG16 model\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Extract features from the image using VGG16 model\n",
    "    features_extracted = model.predict(img_array)\n",
    "\n",
    "    # Add the features and label to the lists\n",
    "    features.append(features_extracted)\n",
    "    label_name.append(label)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "features = np.array(features)\n",
    "label_name = np.array(label_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cf9e2",
   "metadata": {},
   "source": [
    "Note: This step require hardware and time base on size of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e4704",
   "metadata": {},
   "source": [
    "Reshape features and encode lables as integers for write to file features.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ea34558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the feature vectors\n",
    "features = features.reshape((features.shape[0], features.shape[2]))\n",
    "\n",
    "# Encode the labels as integers\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(label_name)\n",
    "\n",
    "# Save the features and labels to an HDF5 file\n",
    "with h5py.File('features.h5', 'w') as hf:\n",
    "    hf.create_dataset('features', data=features)\n",
    "    hf.create_dataset('labels', data=labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280f93ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16854,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008bdf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16854, 1, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432226af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
